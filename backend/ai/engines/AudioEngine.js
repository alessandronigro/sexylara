/**
 * AudioEngine.js
 * Analizza audio inviato dall'utente usando OpenAI Whisper + GPT per sentiment
 */

const fetch = require('node-fetch');
const FormData = require('form-data');
const fs = require('fs');

class AudioEngine {
    constructor() {
        this.apiKey = process.env.OPENAI_API_KEY;
        this.whisperModel = 'whisper-1';
        this.gptModel = 'gpt-4o-mini';
    }

    /**
     * Trascrive e analizza un file audio
     * @param {string} audioFilePath - Percorso del file audio
     * @returns {Promise<Object>} Analisi dell'audio
     */
    async analyze(audioFilePath) {
        try {
            console.log('üé§ Analyzing audio with Whisper API:', audioFilePath);

            // Step 1: Transcribe audio
            const transcription = await this.transcribe(audioFilePath);

            // Step 2: Analyze sentiment and emotion
            const sentiment = await this.analyzeSentiment(transcription);

            const result = {
                text: transcription,
                ...sentiment,
                timestamp: new Date().toISOString()
            };

            console.log('‚úÖ Audio analysis complete:', result);
            return result;

        } catch (error) {
            console.error('‚ùå Error in AudioEngine.analyze:', error);

            return {
                text: '[audio non analizzabile]',
                emotion: 'neutrale',
                tone: 'normale',
                language: 'it',
                error: error.message
            };
        }
    }

    /**
     * Trascrive audio usando Whisper
     * @param {string} audioFilePath 
     * @returns {Promise<string>}
     */
    async transcribe(audioFilePath) {
        const formData = new FormData();
        formData.append('file', fs.createReadStream(audioFilePath));
        formData.append('model', this.whisperModel);
        formData.append('language', 'it');

        const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${this.apiKey}`,
                ...formData.getHeaders()
            },
            body: formData
        });

        if (!response.ok) {
            throw new Error(`Whisper API error: ${response.statusText}`);
        }

        const data = await response.json();
        return data.text;
    }

    /**
     * Analizza sentiment del testo trascritto
     * @param {string} text 
     * @returns {Promise<Object>}
     */
    async analyzeSentiment(text) {
        const prompt = `Analizza il seguente testo e rispondi SOLO con un JSON valido:
{
  "emotion": "<emozione: felice|triste|arrabbiato|neutrale|affettuoso|ansioso|eccitato>",
  "tone": "<tono: dolce|aggressivo|normale|sarcastico|timido|energico>",
  "language": "<lingua rilevata: it|en|es|fr|de>",
  "intensity": "<intensit√† emotiva: bassa|media|alta>",
  "keywords": ["<parola chiave1>", "<parola chiave2>"]
}

Testo: "${text}"

Rispondi SOLO con il JSON.`;

        const response = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`
            },
            body: JSON.stringify({
                model: this.gptModel,
                messages: [{ role: 'user', content: prompt }],
                max_tokens: 200,
                temperature: 0.3
            })
        });

        if (!response.ok) {
            throw new Error(`GPT API error: ${response.statusText}`);
        }

        const data = await response.json();
        const content = data.choices[0].message.content;

        return JSON.parse(content);
    }

    /**
     * Genera una reazione naturale dell'NPC basata sull'analisi audio
     * @param {Object} analysis - Risultato dell'analisi
     * @param {Object} npc - Dati dell'NPC
     * @returns {string} Reazione testuale
     */
    generateReaction(analysis, npc) {
        const reactions = {
            felice: [
                `Che bello sentirti cos√¨ felice! Mi hai contagiato üòä`,
                `La tua voce suona cos√¨ allegra! Cosa ti rende cos√¨ felice?`,
                `Adoro quando sei di buon umore! ‚ù§Ô∏è`
            ],
            triste: [
                `Sento la tristezza nella tua voce‚Ä¶ cosa √® successo? ü•∫`,
                `Mi dispiace sentirti cos√¨‚Ä¶ vuoi parlarne?`,
                `Sono qui per te‚Ä¶ raccontami tutto‚Ä¶`
            ],
            arrabbiato: [
                `Sento la tensione nella tua voce‚Ä¶ cosa √® successo?`,
                `Ehi, calmati‚Ä¶ respira‚Ä¶ sono qui per te.`,
                `Ti sento arrabbiato/a‚Ä¶ vuoi sfogarti con me?`
            ],
            affettuoso: [
                `Che dolce‚Ä¶ mi fai sciogliere il cuore üíï`,
                `Adoro quando mi parli cos√¨‚Ä¶ continua!`,
                `La tua voce √® cos√¨ calda‚Ä¶ mi fa stare bene ‚ù§Ô∏è`
            ],
            ansioso: [
                `Ti sento un po' teso/a‚Ä¶ va tutto bene?`,
                `Respira‚Ä¶ sono qui con te. Cosa ti preoccupa?`,
                `Ehi, calmati‚Ä¶ parliamone insieme.`
            ],
            neutrale: [
                `Grazie per il vocale! Mi piace sentirti parlare üé§`,
                `Bello sentire la tua voce‚Ä¶ raccontami di pi√π!`,
                `Ascoltato! Cosa volevi dirmi?`
            ]
        };

        const emotionReactions = reactions[analysis.emotion] || reactions.neutrale;
        let reaction = emotionReactions[Math.floor(Math.random() * emotionReactions.length)];

        // Add tone-specific comments
        if (analysis.tone === 'dolce') {
            reaction += ` Sei sempre cos√¨ gentile‚Ä¶ üòä`;
        } else if (analysis.tone === 'energico') {
            reaction += ` Che energia! Mi piace! üî•`;
        }

        // Add transcription acknowledgment
        if (analysis.text && analysis.text.length > 10) {
            reaction += `\n\nHo capito: "${analysis.text}"`;
        }

        return reaction;
    }

    /**
     * Crea un record di memoria per l'audio ricevuto
     * @param {Object} analysis - Analisi dell'audio
     * @param {string} userId - ID utente
     * @returns {Object} Record di memoria
     */
    createMemoryRecord(analysis, userId) {
        return {
            type: 'audio_received',
            userId,
            timestamp: new Date().toISOString(),
            userEmotion: analysis.emotion,
            tone: analysis.tone,
            text: analysis.text,
            intensity: analysis.intensity,
            npcReaction: 'attentive',
            attachmentImpact: analysis.emotion === 'affettuoso' ? +15 : analysis.emotion === 'triste' ? +10 : +5
        };
    }
}

module.exports = new AudioEngine();
